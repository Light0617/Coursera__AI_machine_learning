1. 
Suppose that you have trained a logistic regression classifier, 
and it outputs on a new example x a prediction h£c(x) = 0.2. 
This means (check all that apply):

Our estimate for P(y=1|x;¿) is 0.2.
Our estimate for P(y=0|x;¿) is 0.8.

2. 
Suppose you have the following training set, and fit a logistic 
regression classifier h£c(x)=g(£c0+£c1x1+£c2x2).
Which of the following are true? Check all that apply.

Adding polynomial features (e.g., instead using h£c(x)=g(£c0+£c1x1+£c2x2+£c3x21+£c4x1x2+£c5x22) )
could increase how well we can fit the training data.(O)

At the optimal value of £c (e.g., found by fminunc), we will have J(£c)>=0.(O)

Adding polynomial features (e.g., instead using h£c(x)=g(£c0+£c1x1+£c2x2+£c3x21+£c4x1x2+£c5x22) ) 
would increase J(£c) because we are now summing over more terms.(X)

If we train gradient descent for enough iterations,
for some examples x(i) in the training set it is possible to obtain h£c(x(i))>1.(X)

The positive and negative examples cannot be separated using a straight line. So, gradient descent will fail to converge. (X)

Because the positive and negative examples cannot be separated using a straight line, linear regression will perform as well as logistic regression on this data.(O)
XX XO OO =>
3. 
For logistic regression, the gradient is given by ??£cjJ(£c)=1m?mi=1(h£c(x(i))?y(i))x(i)j. 
Which of these is a correct gradient descent update for logistic regression with a learning rate of £\? 
Check all that apply.

£c:=£c?£\1m?mi=1(11+e?£cTx(i)?y(i))x(i).
£c:=£c?£\1m?mi=1(h£c(x(i))?y(i))x(i).

4.(X)
Which of the following statements are true? Check all that apply.

The one-vs-all technique allows you to use logistic regression for problems 
in which each y(i) comes from a fixed, discrete set of values.(O)
 
For logistic regression, sometimes gradient 
descent will converge to a local minimum (and fail to find the global minimum). 
This is the reason we prefer more advanced 
optimization algorithms such as fminunc (conjugate gradient/BFGS/L-BFGS/etc).(X)

Since we train one classifier when there are two classes,
we train two classifiers when there are three classes (and we do one-vs-all classification).(X)

Linear regression always works well for classification if you classify by using a threshold on the prediction made by linear regression.(X)

The cost function J(£c) for logistic regression trained with m?
1 examples is always greater than or equal to zero.(O)
5. 
Suppose you train a logistic classifier h£c(x)=g(£c0+£c1x1+£c2x2). 
Suppose £c0=?6,£c1=1,£c2=0. Which of the following figures represents the 
decision boundary found by your classifier?

x1=6
